{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 4: Análisis Avanzado y Flujo de Diálogo\n",
    "\n",
    "Este notebook cubre las Actividades 4 a 7 de la Ruta de Aprendizaje Autónomo 3:\n",
    "4.  **Generación de Vectores**: Word2Vec vs S-BERT.\n",
    "5.  **Análisis Comparativo**: Clustering de tópicos y Analogías.\n",
    "6.  **Flujo de Diálogo**: Integración con LLM (Ollama).\n",
    "7.  **Pruebas**: Simulación de consultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Embeddings & Clustering\n",
    "from gensim.models import Word2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap.umap_ as umap\n",
    "import hdbscan\n",
    "\n",
    "# LangChain & LLM\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/processed_corpus.csv\")\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna(subset=['clean_text'])\n",
    "\n",
    "print(f\"Corpus Total: {len(df)} documentos\")\n",
    "texts = df['clean_text'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Actividad 4: Generación de Vectores (Word2Vec vs S-BERT)\n",
    "\n",
    "Entrenaremos un modelo `Word2Vec` desde cero (captura relaciones sintácticas específicas del corpus) y usaremos `S-BERT` pre-entrenado (captura semántica profunda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === A. Word2Vec (Entrenamiento Local) ===\n",
    "print(\"Entrenando Word2Vec...\")\n",
    "tokenized_texts = [t.split() for t in texts]\n",
    "w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=2, workers=4)\n",
    "print(\"Word2Vec entrenado.\")\n",
    "\n",
    "# === B. S-BERT (Pre-entrenado) ===\n",
    "print(\"Cargando S-BERT (all-MiniLM-L6-v2)...\")\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings_sbert = sbert_model.encode(texts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Actividad 5: Análisis Comparativo (Analogías y Clustering)\n",
    "\n",
    "### 3.1 Analogías con Word2Vec\n",
    "Intentamos resolver relaciones semánticas tipo: *Rey - Hombre + Mujer = Reina*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analogy(w1, w2, w3):\n",
    "    try:\n",
    "        result = w2v_model.wv.most_similar(positive=[w1, w3], negative=[w2], topn=1)\n",
    "        print(f\"{w1} - {w2} + {w3} = {result[0][0]} ({result[0][1]:.2f})\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Palabras no encontradas en vocabulario ({e})\")\n",
    "\n",
    "# Prueba con términos hipotéticos del dominio urbano (ajustar según vocabulario real)\n",
    "print(\"--- Pruebas de Analogía ---\")\n",
    "# Ejemplo: Si 'calle' es a 'asfalto', 'acera' es a...?\n",
    "test_analogy('calle', 'coche', 'acera') \n",
    "# Ejemplo: 'suciedad' - 'basura' ...\n",
    "test_analogy('ruido', 'trafico', 'basura')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Discovery de Tópicos (Clustering con UMAP + HDBSCAN)\n",
    "Usamos los embeddings de S-BERT para encontrar grupos semánticos automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reducción de Dimensionalidad (UMAP)\n",
    "print(\"Reduciendo dimensiones con UMAP...\")\n",
    "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
    "                            n_components=2, \n",
    "                            metric='cosine').fit_transform(embeddings_sbert)\n",
    "\n",
    "# 2. Clustering (HDBSCAN)\n",
    "print(\"Clusterizando con HDBSCAN...\")\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=15,\n",
    "                            metric='euclidean',\n",
    "                            cluster_selection_method='eom')\n",
    "cluster_labels = clusterer.fit_predict(umap_embeddings)\n",
    "\n",
    "df['cluster'] = cluster_labels\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "print(f\"Clusters encontrados: {n_clusters} (Ruido asignado a -1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=cluster_labels, cmap='Spectral', s=5)\n",
    "plt.colorbar(label='Cluster ID')\n",
    "plt.title('Proyección UMAP de Quejas Ciudadanas (S-BERT)')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspección de Clusters (Top palabras por cluster)\n",
    "# Una forma simple es ver los textos más cercanos al centroide o textos aleatorios\n",
    "for i in range(n_clusters):\n",
    "    print(f\"\\n--- Cluster {i} ---\")\n",
    "    sample_docs = df[df['cluster'] == i]['clean_text'].sample(5).values\n",
    "    for doc in sample_docs:\n",
    "        print(f\"- {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Actividad 6 y 7: Flujo de Diálogo & RAG (LangChain + Ollama)\n",
    "\n",
    "Simularemos un asistente inteligente que recupera documentos relevantes (RAG) y genera respuestas usando `gemma3:1b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar LLM Local\n",
    "llm = ChatOllama(model=\"gemma3:1b\", temperature=0.3)\n",
    "\n",
    "# --- Simple Retriever (basado en Coseno con S-BERT) ---\n",
    "def retrieve_documents(query, k=3):\n",
    "    query_vec = sbert_model.encode([query])\n",
    "    sims = cosine_similarity(query_vec, embeddings_sbert)[0]\n",
    "    # Top k indices\n",
    "    top_indices = sims.argsort()[-k:][::-1]\n",
    "    return df.iloc[top_indices]['clean_text'].tolist()\n",
    "\n",
    "# --- Chain Definition ---\n",
    "prompt_template = \"\"\"\n",
    "Eres un asistente de IA para una plataforma de gestión urbana. \n",
    "Usa el siguiente contexto (quejas reales de ciudadanos) para responder a la pregunta del usuario. \n",
    "Si no sabes la respuesta basada en el contexto, dilo claramente.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta Ciudadana: {question}\n",
    "\n",
    "Respuesta Útil y Empática:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "def run_rag(question):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Retrieve\n",
    "    docs = retrieve_documents(question)\n",
    "    context_str = \"\\n\".join([f\"- {d}\" for d in docs])\n",
    "    \n",
    "    # 2. Generate\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    response = chain.invoke({\"context\": context_str, \"question\": question})\n",
    "    \n",
    "    latency = time.time() - start_time\n",
    "    return response, context_str, latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simulación de Consultas (Actividad 7) ---\n",
    "test_queries = [\n",
    "    \"¿Cuáles son las quejas más comunes sobre el tráfico?\",\n",
    "    \"Hay mucha basura en las calles, ¿qué dice la gente?\",\n",
    "    \"¿Es segura la zona del centro por la noche?\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Pregunta: {q}\")\n",
    "    ans, ctx, lat = run_rag(q)\n",
    "    print(f\"Latencia: {lat:.2f}s\")\n",
    "    print(f\"Contexto Recuperado (Snippet): {ctx[:200]}...\")\n",
    "    print(f\"\\nRespuesta IA:\\n{ans}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
