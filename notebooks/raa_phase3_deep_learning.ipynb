{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fase 3b: Deep Learning Models (BiLSTM)\n",
                "\n",
                "En este notebook, usamos `Word2Vec` para entrenar embeddings y `BiLSTM` para la clasificaci√≥n."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "import sys\n",
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "\n",
                "# Add src\n",
                "sys.path.append(os.path.abspath(\"../src\"))\n",
                "from dl_models import AdvancedDLManager"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train: 94186, Test: 23547\n"
                    ]
                }
            ],
            "source": [
                "data_path = Path(\"../data/processed_corpus.csv\")\n",
                "df_full = pd.read_csv(data_path)\n",
                "df_full = df_full.dropna(subset=['clean_text', 'sentiment_score'])\n",
                "\n",
                "# Definimos columnas a probar\n",
                "input_columns = ['clean_text', 'lemmas_text']\n",
                "\n",
                "# Usamos clean_text solo para sacar los √≠ndices, luego usaremos la columna que toque\n",
                "X_indices = df_full['clean_text'] \n",
                "y = df_full['sentiment_score']\n",
                "\n",
                "# Test Set Intocable\n",
                "X_train_raw, X_test_real, y_train_raw, y_test_real = train_test_split(\n",
                "    X_indices, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "train_idx = X_train_raw.index\n",
                "test_idx = X_test_real.index\n",
                "\n",
                "print(f\"Indices fijados -> Train Total: {len(train_idx)}, Test Intocable: {len(test_idx)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Entrenamiento de modelos de deep learning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "experiments_config = [\n",
                "    {\n",
                "        'name': 'Baseline: Word2Vec + BiLSTM',\n",
                "        'strategy': 'w2v',\n",
                "        'model': None\n",
                "    },\n",
                "    {\n",
                "        'name': 'SOTA: All-MiniLM + MLP',\n",
                "        'strategy': 'transformer',\n",
                "        'model': 'sentence-transformers/all-MiniLM-L6-v2' \n",
                "    },\n",
                "    {\n",
                "        'name': 'SOTA: BGE-Small + MLP',\n",
                "        'strategy': 'transformer',\n",
                "        'model': 'BAAI/bge-small-en-v1.5'\n",
                "    },\n",
                "    {\n",
                "        'name': 'GenAI: Gemma-Embed + MLP',\n",
                "        'strategy': 'ollama',\n",
                "        'model': 'embeddinggemma:latest'\n",
                "    }\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n",
                        "Training Word2Vec...\n",
                        "Training Word2Vec...\n",
                        "Word2Vec trained.\n",
                        "Training BiLSTM...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/5:   3%|‚ñé         | 41/1325 [00:11<05:49,  3.68it/s]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=\u001b[32m0.1\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=y_train)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining BiLSTM...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = \u001b[43mdl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\otero\\Documents\\PKM\\200 - üåç BEREICHE - AREAS\\UNIVERSIT√ÑT - Universidad\\UTAMED\\PLN\\raa\\src\\dl_models.py:110\u001b[39m, in \u001b[36mDLManager.train_model\u001b[39m\u001b[34m(self, X_train, y_train, X_val, y_val, epochs, batch_size, lr)\u001b[39m\n\u001b[32m    108\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.model(feats)\n\u001b[32m    109\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m optimizer.step()\n\u001b[32m    112\u001b[39m train_loss += loss.item()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\otero\\Documents\\PKM\\200 - üåç BEREICHE - AREAS\\UNIVERSIT√ÑT - Universidad\\UTAMED\\PLN\\raa\\.venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\otero\\Documents\\PKM\\200 - üåç BEREICHE - AREAS\\UNIVERSIT√ÑT - Universidad\\UTAMED\\PLN\\raa\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\otero\\Documents\\PKM\\200 - üåç BEREICHE - AREAS\\UNIVERSIT√ÑT - Universidad\\UTAMED\\PLN\\raa\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
                        "\u001b[31mKeyboardInterrupt\u001b[39m: "
                    ]
                }
            ],
            "source": [
                "results_dl = []\n",
                "\n",
                "for col in input_columns:\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\">>> PROCESANDO FEATURE: {col.upper()} <<<\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    X_full_col = df_full[col].astype(str) \n",
                "    \n",
                "    X_train_curr = X_full_col.loc[train_idx]\n",
                "    y_train_curr = y.loc[train_idx]\n",
                "    \n",
                "    X_test_curr = X_full_col.loc[test_idx]\n",
                "    y_test_curr = y.loc[test_idx]\n",
                "    \n",
                "    # Creamos DF temporal para facilitar el sampleo\n",
                "    train_df_temp = pd.DataFrame({'feature': X_train_curr, 'target': y_train_curr})\n",
                "    min_c = train_df_temp['target'].value_counts().min()\n",
                "    \n",
                "    print(f\"Balanceando Train a {min_c} muestras por clase...\")\n",
                "    \n",
                "    balanced_train = train_df_temp.groupby('target').apply(\n",
                "        lambda x: x.sample(min_c, random_state=42)\n",
                "    ).reset_index(drop=True)\n",
                "    \n",
                "    X_train_bal = balanced_train['feature']\n",
                "    y_train_bal = balanced_train['target']\n",
                "    \n",
                "    X_tr_final, X_val_final, y_tr_final, y_val_final = train_test_split(\n",
                "        X_train_bal, y_train_bal, test_size=0.1, random_state=42, stratify=y_train_bal\n",
                "    )\n",
                "    \n",
                "    print(f\"   Datos Finales DL -> Train: {len(X_tr_final)}, Val: {len(X_val_final)}\")\n",
                "    \n",
                "    for exp in experiments_config:\n",
                "        exp_id = f\"{exp['name']} ({col})\"\n",
                "        print(f\"\\n   >>> Entrenando: {exp_id}\")\n",
                "        \n",
                "        try:\n",
                "            # Instanciar\n",
                "            dl_man = AdvancedDLManager(strategy=exp['strategy'], model_name=exp['model'])\n",
                "            \n",
                "            # Entrenar W2V (si toca)\n",
                "            if exp['strategy'] == 'w2v':\n",
                "                # Entrenamos W2V con TODO el train balanceado (incluyendo val) para mejor vocabulario\n",
                "                dl_man.train_w2v(X_train_bal)\n",
                "            \n",
                "            # Entrenar Red Neuronal\n",
                "            history = dl_man.train(X_tr_final, y_tr_final, X_val_final, y_val_final, \n",
                "                                 epochs=5, batch_size=32)\n",
                "            \n",
                "            # Evaluar en Test Real\n",
                "            print(\"      Evaluando en Test Set...\")\n",
                "            rep = dl_man.evaluate(X_test_curr, y_test_curr)\n",
                "            \n",
                "            # Guardar\n",
                "            results_dl.append({\n",
                "                'Feature': col,\n",
                "                'Model': exp['name'],\n",
                "                'Report_Raw': rep,\n",
                "                'History': history\n",
                "            })\n",
                "            \n",
                "            # Print r√°pido de resultados\n",
                "            lines = rep.split('\\n')\n",
                "            print(f\"RESULTADO: {lines[-4].strip()} | {lines[-3].strip()}\")\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"ERROR en {exp_id}: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for res in results_dl:\n",
                "    print(f\"\\n[{res['Feature']}] {res['Model']}\")\n",
                "    lines = res['Report_Raw'].split('\\n')\n",
                "    print(f\"   Accuracy: {lines[-4].split()[1]}\")\n",
                "    print(f\"   Macro F1: {lines[-3].split()[-2]}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
