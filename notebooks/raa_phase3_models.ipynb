{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase 3: Model Development (Classification)\n",
                "\n",
                "In this notebook, we will train and evaluate baseline classification models using the **Balanced Corpus** generated in Phase 2.\n",
                "\n",
                "**Objectives:**\n",
                "1. Load `processed_corpus_balanced.csv`.\n",
                "2. Train Baseline Models (Logistic Regression, SVM, Random Forest) using TF-IDF.\n",
                "3. Compare performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "import sys\n",
                "import os\n",
                "from pathlib import Path\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append(os.path.abspath(\"../src\"))\n",
                "from models import SentimentClassifier"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_path = Path(\"../data/processed_corpus_balanced.csv\")\n",
                "df = pd.read_csv(data_path)\n",
                "print(f\"Loaded {len(df)} rows.\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Train/Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop rows with NaN in text or sentiment (just safety)\n",
                "df = df.dropna(subset=['clean_text', 'sentiment_score'])\n",
                "\n",
                "X = df['clean_text']\n",
                "y = df['sentiment_score']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Baseline Experiments\n",
                "We will test: Logistic Regression, SVM, Random Forest."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = {}\n",
                "models = ['logreg', 'svm', 'rf']\n",
                "\n",
                "for m in models:\n",
                "    clf = SentimentClassifier(model_type=m)\n",
                "    clf.train(X_train, y_train)\n",
                "    res = clf.evaluate(X_test, y_test)\n",
                "    results[m] = res"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare Accuracies\n",
                "acc_df = pd.DataFrame([(m, res['accuracy']) for m, res in results.items()], columns=['Model', 'Accuracy'])\n",
                "sns.barplot(data=acc_df, x='Model', y='Accuracy')\n",
                "plt.title('Baseline Model Comparison')\n",
                "plt.ylim(0, 1)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Heatmap for best model (likely SVM or LogReg)\n",
                "best_model = acc_df.sort_values('Accuracy', ascending=False).iloc[0]['Model']\n",
                "print(f\"Best Model: {best_model}\")\n",
                "\n",
                "cm = results[best_model]['confusion_matrix']\n",
                "plt.figure(figsize=(8,6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
                "plt.title(f'Confusion Matrix ({best_model})')\n",
                "plt.ylabel('True')\n",
                "plt.xlabel('Predicted')\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}